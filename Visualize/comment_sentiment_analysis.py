# -*- coding:utf-8 -*-
# @Timeï¼š 2020/7/13 11:48 AM
# @Author: dyf-2316
# @FileName: commet_sentiment_analysis.py
# @Software: PyCharm
# @Project: Comment_Sentiment_Analysis
# @Description: visualize this project
import base64
import json
import os
import subprocess
import time
import webbrowser

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image

plt.rcParams['font.sans-serif'] = ['SimHei']

st.title('åŸºäºpythonçš„ç”µå•†äº§å“è¯„è®ºæ•°æ®æƒ…æ„Ÿåˆ†æ')
st.markdown('ğŸŒˆ ç»„6  ä¸ä¸€å‡¡ :dog: ææ¯… :see_no_evil: é²å«ç«  â˜ ï¸ é©¬ç”Ÿé¸¿ :feet: &ensp; :star2: [GitHub]('
            'https://github.com/dyf-2316/Comment_Sentiment_Analysis) :star2:')


def login(path):
    # webbrowser.get('Safari').open(u"https://dyf-2316.github.io/LDA_Results/" + path)
    webbrowser.open(u"https://dyf-2316.github.io/LDA_Results/" + path)


@st.cache
def load_image(path):
    image = Image.open(path)
    return image


@st.cache
def load_data_origin():
    data_origin = pd.read_csv('data/000_meidi_data_origin.txt', encoding='utf-8')
    return data_origin


@st.cache
def load_data_deldup():
    data_deldup = pd.read_csv('data/001_meidi_data_deldup.txt', encoding='utf-8')
    return data_deldup


@st.cache
def load_data_compress():
    data_compress = pd.read_csv('data/002_meidi_data_comressed.txt', encoding='utf-8')
    return data_compress


@st.cache
def load_tag_comments_origin():
    with open('data/003_meidi_tagComment.json', 'r', encoding='utf-8') as f:
        tag_comments = json.load(f)
    return tag_comments


@st.cache
def load_comments_sentiment():
    data_classify = pd.read_csv('data/004_meidi_data_sentiment.txt', encoding='utf-8')
    return data_classify


@st.cache
def load_neg_comment():
    data = pd.read_csv('data/005_neg_comment.csv', encoding='utf-8', header=None)
    data = data.drop(index=[0])
    data = data.drop(columns=[0])
    return data


@st.cache
def load_pos_comment():
    data = pd.read_csv('data/005_pos_comment.csv', encoding='utf-8', header=None)
    data = data.drop(index=[0])
    data = data.drop(columns=[0])
    return data


@st.cache
def load_tag_comments_pos():
    with open('data/006_tag_pos_comments.json', 'r', encoding='utf-8') as f:
        tag_comments_pos = json.load(f)
    return tag_comments_pos


@st.cache
def load_tag_comments_neg():
    with open('data/006_tag_neg_comments.json', 'r', encoding='utf-8') as f:
        tag_comments_neg = json.load(f)
    return tag_comments_neg


@st.cache
def load_LDA_coherence():
    with open('data/007_LDA_cv_coherence.json', 'r', encoding='utf-8') as f:
        LDA_coherence = json.load(f)
    return LDA_coherence


data_origin = load_data_origin()
data_deldup = load_data_deldup()
data_compress = load_data_compress()
tag_comment_origin = load_tag_comments_origin()
data_comment = load_comments_sentiment()
neg_comment = load_neg_comment()
pos_comment = load_pos_comment()
tag_comments_pos = load_tag_comments_pos()
tag_comments_neg = load_tag_comments_neg()
LDA_coherence = load_LDA_coherence()

st.sidebar.markdown('# é¡¹ç›®ç›®å½•')
section = st.sidebar.radio("è¯·é€‰æ‹©éœ€è¦å±•ç¤ºçš„é¡¹ç›®æ¨¡å—ï¼š",
                           ('0âƒ£ï¸ é¡¹ç›®ä»‹ç»ä¸è§„åˆ’', '1âƒ£ï¸ æ•°æ®é‡‡é›†ä¸æŠ½å–', '2âƒ£ï¸ æ•°æ®é¢„å¤„ç†ä¸æ¢ç´¢', '3âƒ£ï¸ è‡ªè®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹', '4âƒ£ï¸ è¯„è®ºåˆ†è¯ä¸æ”¹è¿›',
                            '5âƒ£ï¸ è¯äº‘ä¸è¯­ä¹‰ç½‘ç»œæ„å»º', '6âƒ£ï¸ LDAä¸»é¢˜æ¨¡å‹æ„å»º', '7âƒ£ï¸ äº¤äº’è¯Šæ–­ä¸åé¦ˆ'))

tag = None
if section in ['5âƒ£ï¸ è¯äº‘ä¸è¯­ä¹‰ç½‘ç»œæ„å»º', '6âƒ£ï¸ LDAä¸»é¢˜æ¨¡å‹æ„å»º', '7âƒ£ï¸ äº¤äº’è¯Šæ–­ä¸åé¦ˆ']:
    st.sidebar.markdown('# è¯„è®ºæ ‡ç­¾')
    tag = st.sidebar.selectbox("  è¯·é€‰æ‹©åˆ†æçš„è¯„è®ºæ ‡ç­¾ï¼š",
                               ('æ€»ä½“è¯„è®º', 'å¤–å½¢å¤–è§‚', 'æ’æ¸©æ•ˆæœ', 'å™ªéŸ³å¤§å°', 'å‡ºæ°´é€Ÿåº¦', 'å®‰è£…æœåŠ¡', 'è€—èƒ½æƒ…å†µ', 'åŠ çƒ­é€Ÿåº¦', 'æ´—æµ´æ—¶é—´', 'å…¶ä»–ç‰¹è‰²'))

topic_number = 5
if section in ['6âƒ£ï¸ LDAä¸»é¢˜æ¨¡å‹æ„å»º']:
    st.sidebar.markdown('# LDAä¸»é¢˜æ•°')
    topic_number = st.sidebar.slider('  è¯·é€‰æ‹©éœ€è¦è®­ç»ƒçš„LDAä¸»é¢˜æ•°:', 3, 9, 5)

if section == '0âƒ£ï¸ é¡¹ç›®ä»‹ç»ä¸è§„åˆ’':
    st.markdown('## 0. é¡¹ç›®ä»‹ç»ä¸è§„åˆ’')

    st.markdown('***')
    st.markdown('### 0.1 å¼€å‘äººå‘˜æ¸…å•ä¸åˆ†å·¥')
    st.markdown("""- ä¸ä¸€å‡¡ï¼š
    1. æ•°æ®çˆ¬å–ä¸æ•°æ®æŠ½å–
    2. æ•°æ®é¢„å¤„ç†ä»¥åŠæ ‡ç­¾è¯„è®ºçš„å¤„ç†
    3. æ’°å†™ä¼šè®®è®°å½•
    4. Streamlitå¯è§†åŒ–å‘ˆç°æœ€ç»ˆç»“æœ
    5. å°†é¡¹ç›®æ­å»ºåœ¨Herokuäº‘æœåŠ¡å™¨ä¸Š
- é²å«ç« ï¼š
    1. å¯¹æ•´ä½“è¯„è®ºæ•°æ®ä¸åˆ†æ ‡ç­¾è¯„è®ºæ•°æ®è¿›è¡ŒLDAä¸»é¢˜åˆ†ææ¨¡å‹å»ºæ¨¡åŠå¯è§†åŒ–
    2. ä½¿ç”¨ROSTå¯¹åˆ†æ ‡ç­¾è¯„è®ºæ•°æ®è¿›è¡Œè¯­ä¹‰ç½‘ç»œåˆ†æ
    3. éœ€æ±‚æ–‡æ¡£æ’°å†™åŠæ›´æ–°ç­‰
- é©¬ç”Ÿé¸¿ï¼š
    1. å¯¹æ•´ä½“è¯„è®ºæ•°æ®å’Œåˆ†æ ‡ç­¾è¯„è®ºæ•°æ®è¿›è¡Œè¯äº‘çš„ç»˜åˆ¶
    2. æ’°å†™ç›¸å…³æ–‡æ¡£
- ææ¯…: 
    1. ä½¿ç”¨RoBERTa - wwm - exté¢„è®­ç»ƒç½‘ç»œå®ç°æ–‡æœ¬æƒ…æ„Ÿé¢„æµ‹ï¼Œå¯¹æ¯”ROSTé¢„æµ‹æ­£ç¡®ç‡æå‡25 %ï¼Œå¹¶å¯¹ä¸åŒçš„æ ‡ç­¾çš„è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æ
    2. å®Œæˆæ¨¡å‹éƒ¨åˆ†å¯è§†åŒ–
    3. å°†é¡¹ç›®æ­å»ºåœ¨Herokuäº‘æœåŠ¡å™¨ä¸Š
    4. å®Œæˆé¡¹ç›®å±•ç¤ºæŠ¥å‘Šï¼Œé¡¹ç›®ç«‹é¡¹ä¹¦ç­‰

    """)

    st.markdown('***')
    st.markdown('### 0.2 é¡¹ç›®å¼€å‘ç¯å¢ƒæ¸…å•')
    environment_list = pd.DataFrame(
        {'åç§°': ['Windows10 & MacBook Pro', 'pycharm + google colab', 'python3.7', 'MongoDB', 'GitHub', 'Heroku'],
         'ç¯å¢ƒç»†èŠ‚': ['é¡¹ç›®å¼€å‘ç¡¬ä»¶ç¯å¢ƒ', 'é¡¹ç›®å¼€å‘æ‰€ç”¨IDEä¸è¿œç¨‹jupyter', 'pythonçš„ç‰ˆæœ¬', 'æ•°æ®å­˜å‚¨çš„æ•°æ®åº“', 'é¡¹ç›®ç‰ˆæœ¬ç®¡ç†', 'é¡¹ç›®å±•ç¤ºå¹³å°']
         })
    st.table(environment_list)

    st.markdown('***')
    st.markdown('### 0.3 é¡¹ç›®å¼€å‘å‘¨æœŸè¡¨')
    image = load_image('data/source/develop_schedule.png')
    st.image(image, use_column_width=True)

    st.markdown('***')
    st.markdown('### 0.4 é¡¹ç›®å¼€å‘æµç¨‹å›¾')
    image = load_image('data/source/flow_chart.png')
    st.image(image, use_column_width=True)

    st.markdown('***')
    st.markdown('### 0.5 é¡¹ç›®å®ç”¨æŠ€æœ¯')
    tech_list = pd.DataFrame(
        {'æ¨¡å—': ['æ•°æ®çˆ¬å–', 'æ•°æ®å­˜å‚¨', 'æ•°æ®é¢„å¤„ç†', 'éšå«ä¿¡æ¯æŒ–æ˜', 'æƒ…æ„Ÿåˆ†æ', 'è¯„è®ºåˆ†è¯', 'è¯äº‘ä¸è¯­ä¹‰ç½‘ç»œ', 'LDAä¸»é¢˜æ¨¡å‹', 'æ¨¡å‹è¯„ä¼°ä¸è¯Šæ–­', 'å¯è§†åŒ–'],
         'ç¯å¢ƒç»†èŠ‚': ['ä½¿ç”¨urllibè·å–æ•°æ®ï¼Œå¹¶å¯¹urlè¿›è¡Œç²¾ç¡®è§£æ', 'å°†æ•°æ®å­˜å‚¨åœ¨MongoDBä¸­', 'æ•°æ®å»é‡ã€æœºæ¢°å‹ç¼©ã€çŸ­å¥å»é™¤',
                  'æå–å¸¦æ ‡ç­¾çš„è¯„è®º', 'åˆ†è¯ã€å»é™¤åœç”¨è¯ã€è¯æ€§è¿‡æ»¤ï¼ˆåªå–åŠ¨å½¢åç­‰å®è¯ï¼‰', 'ä½¿ç”¨å“ˆå·¥å¤§å…¬å¼€çš„é¢„è®­ç»ƒç½‘ç»œRoBERTa-wwm-extè¿›è¡Œæ–‡æœ¬æƒ…æ„Ÿé¢„æµ‹',
                  'å¯¹ä¸åŒæ ‡ç­¾çš„æ­£è´Ÿé¢è¯„è®ºåˆ†åˆ«ä½¿ç”¨è¯äº‘å’ŒROSTè¿›è¡Œå‰ªæè¯­ä¹‰ç½‘ç»œåˆ†æ', 'ä½¿ç”¨pyLDAvisè¿›è¡ŒåŠ¨æ€å¯è§†åŒ–ï¼Œå®ç°è¾ƒå¥½çš„äº¤äº’æ€§ï¼Œå¹¶è°ƒèŠ‚å‚æ•°å¾—åˆ°æœ€ä¼˜çš„ä¸»é¢˜æå–',
                  'é’ˆå¯¹ä¸åŒæ ‡ç­¾çš„è¯„è®ºå®šå‘åˆ†æäº§å“ä¸åŒæ–¹é¢çš„ä¼˜åŠ£åŠ¿åŠå…¶å–ç‚¹', 'ä½¿ç”¨streamlitå°†é¡¹ç›®å±•ç¤ºåœ¨ç½‘é¡µä¸Šï¼Œå¹¶è¿è¡ŒäºHerokuäº‘æœåŠ¡å™¨ä¸Š']
         })
    st.table(tech_list)

    st.markdown('***')
    st.markdown('### 0.6 é‡åˆ°çš„é—®é¢˜ä»¥åŠè§£å†³æ–¹æ³•')
    st.markdown(' - å¤šäººä½¿ç”¨gitç‰ˆæœ¬ç®¡ç†ä¸­é‡åˆ°æ–‡ä»¶å†²çªçš„é—®é¢˜')
    st.markdown('è§£å†³åŠæ³•ï¼šä»ç½‘ä¸ŠæŸ¥æ‰¾èµ„æ–™ï¼Œå›é€€ç‰ˆæœ¬')
    st.markdown(' - çˆ¬è™«æ•°æ®è´¨é‡å·®ï¼Œæ•°æ®é‡å°‘ï¼Œå·®è¯„æ¯”ä¾‹å°')
    st.markdown('è§£å†³åŠæ³•ï¼šè§‚å¯Ÿurlæ ¼å¼éå†å‚æ•°å€¼æ¥å°½å¯èƒ½è·å–å¤šçš„æ•°æ®é‡ã€‚')
    st.markdown(' - é”™è¯¯æ ¼å¼ä»¥åŠè®¿é—®è¿‡å¿«ä½¿å¾—æ•´ä¸ªçˆ¬è™«è¿›ç¨‹ä¸­æ–­')
    st.markdown('è§£å†³åŠæ³•ï¼šåŠ å…¥æ—¥è®°çš„è®°å½•ï¼Œä»¥åŠå®Œå–„å¼‚å¸¸å¤„ç†æœºåˆ¶')
    st.markdown(' - å…¨é¢çš„æå–æ ‡ç­¾æ•°æ®ï¼Œä»¥åŠæ ‡ç­¾æ•°æ®çš„å­˜å‚¨')
    st.markdown('è§£å†³åŠæ³•ï¼šè§‚å¯Ÿå­—ç¬¦ä¸²çš„è§„èŒƒï¼Œè®¾è®¡ç®—æ³•å…¨é¢æå–æ ‡ç­¾æ•°æ®ï¼Œè€ƒè™‘æ ‡ç­¾æ•°æ®ä¸ç­‰é•¿ï¼Œé‡‡ç”¨å­—å…¸å½¢å¼å¹¶jsonå­˜å‚¨')
    st.markdown(' - åœ¨ä½¿ç”¨RoBERTa-wwm-exté¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ä¸­æ¨¡å‹lossä¸æ”¶æ•›')
    st.markdown('è§£å†³åŠæ³•ï¼šæ¢ç”¨ä¸åŒçš„ä¼˜åŒ–å™¨å’Œè¶…å‚æ•°')
    st.markdown(' - åœ¨tensorboardä½¿ç”¨æ—¶é¢‘ç¹æŠ¥é”™')
    st.markdown('è§£å†³åŠæ³•ï¼šæ”¹ä¸ºä½¿ç”¨tensorboardX')
    st.markdown(' - åˆ†è¯æ•ˆæœè´¨é‡å·®ï¼Œä½¿å¾—ä¸»é¢˜æå–ä¸­é«˜é¢‘å…³é”®è¯è¯­ä¹‰è¡¨è¾¾æ¨¡ç³Š')
    st.markdown('è§£å†³åŠæ³•ï¼šåœ¨åˆ†è¯ä¸­è€ƒè™‘è¯æ€§ï¼Œæ ¹æ®ä¿ç•™ä¸åŒçš„è¯æ€§é›†æ‰€å¾—åˆ°LADç»“æœè´¨é‡ï¼Œæœ€ç»ˆé€‰æ‹©åªä¿ç•™åŠ¨å½¢åè¯')
    st.markdown(' - ç”Ÿæˆè¯äº‘å›¾ç‰‡çš„åˆ†è¾¨ç‡ä¸é«˜')
    st.markdown('è§£å†³åŠæ³•ï¼šè°ƒæ•´å›¾ç‰‡å¤§å°å’Œè¯æ±‡å¤§å°')
    st.markdown(' - å¯¹äºNegativeçš„æ ‡ç­¾æ•°æ®è¿›è¡Œldaä¸»é¢˜åˆ†æï¼Œå‡ºç°æ¨¡å‹è¿è¡Œé”™è¯¯')
    st.markdown('è§£å†³åŠæ³•ï¼šNegativeçš„æ ‡ç­¾æ•°æ®é‡è¾ƒå°‘ï¼Œå‡å°‘ä¸»é¢˜æ•°é‡ï¼Œé‡æ–°å»ºæ¨¡')
    st.markdown(' - é¡¹ç›®éƒ¨ç½²åˆ°è¿œç«¯æœåŠ¡å™¨åæ— æ³•å¯¹htmlæ–‡ä»¶åœ¨æµè§ˆå™¨ä¸­è®¿é—®')
    st.markdown('è§£å†³åŠæ³•ï¼šå°†æ–‡ä»¶éƒ¨ç½²åˆ°github.ioä¸Šå¯ä»¥ç›´æ¥ä½¿ç”¨urlå¯¹èµ„æºè®¿é—®')
    st.markdown('***')

if section == '1âƒ£ï¸ æ•°æ®é‡‡é›†ä¸æŠ½å–':
    st.markdown('## 1. æ•°æ®é‡‡é›†ä¸æŠ½å–')

    st.markdown('***')
    st.markdown('### 1.1 æ•°æ®çˆ¬å–')
    st.markdown(' - è¯¥é¡¹ç›®åœ¨äº¬ä¸œå•†åŸçˆ¬å–ç¾çš„çƒ­æ°´å™¨å“ç‰ŒåŸå§‹æ•°æ®ï¼Œå…±è®¡ {} æ¡'.format(len(data_origin)))
    st.markdown(' - æ•°æ®æ¡†ï¼ˆä¸‹æ–¹å±•ç¤ºï¼‰ä¸­åŒ…å«\n'
                '   - äº§å“å‹å·(good_id)\n'
                '   - å“ç‰Œåç§°(brand)\n'
                '   - ä»·æ ¼(price)\n'
                '   - è¯„è®º(comment)\n'
                '   - è¯„è®ºæ—¥æœŸ(creationDate)')
    if st.checkbox('Show origin data'):
        st.dataframe(data_origin, 900, 400)

    code_webcrawler = '''
    COMMENT_URL = "https://club.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98&productId={}&score={}&sortType=5&page={}&pageSize=10&isShadowSku=0&rid=0&fold=1 "

    def get_comment_data(product_id, score, page, product):
        """
        è·å–product_idç›¸åº”scoreå’Œpageçš„è¯„è®ºæ•°æ®ï¼Œä¸å•†å“åŸºæœ¬ä¿¡æ¯ä¸€èµ·æ„æˆæ•°æ®å­—å…¸è¿”å›
        :param product_id: (str) å•†å“ID
        :param score: (int) å•†å“è¯„åˆ†
        :param page: (int) è¯„è®ºé¡µæ•°
        :param product: (dict) å•†å“åŸºæœ¬ä¿¡æ¯çš„æ•°æ®å­—å…¸
        :return: (dict) å•†å“ä¿¡æ¯åŠè¯„è®ºæ•°æ®å­—å…¸
        """
        comment_url = COMMENT_URL.format(product_id, score, page)
        mylogger.debug("è·å–å•†å“è¯„è®ºé¡µé¢URL -- {}".format(comment_url))

        json_data = get_json_data(comment_url)
        data = json_data['comments']
        comment_data = []
        for i in range(len(data)):
            comment = {
                "good_id": product['good_id'],
                "brand": product['brand'],
                "price": product['price'],
                "creationTime": data[i]['creationTime'],
                "score": data[i]['score'],
                "comment": data[i]['content']
            }
            comment_data.append(comment)
        mylogger.debug("è·å–å•†å“è¯„è®ºæ•°æ® -- counts:{}".format(len(comment_data)))
        return comment_data
    '''
    st.code(code_webcrawler)

    st.markdown('***')
    st.markdown('### 1.2 æ•°æ®å­˜å‚¨ï¼ˆMongoDBï¼‰')
    code_save_mongo = '''
    def save_to_mongo(data):
    """
    å°†æ•°æ®å­˜å…¥mongoDB
    :param data: (dict) éœ€è¦å­˜å…¥æ•°æ®åº“çš„æ•°æ®å­—å…¸
    :return:
    """
    client = MongoClient(**DATABASE_MONGO)
    mongo_db = client['WebCrawler']
    mongo_table = mongo_db['comment']
    for item in data:
        mongo_table.collection.insert_one(item)
        mylogger.debug("å­˜å…¥æ•°æ® -- {}".format(item))
    '''
    st.code(code_save_mongo)

    st.markdown('***')
    st.markdown('### 1.3 è¯„è®ºæŠ½å–')
    st.markdown('ä»åŸå§‹ä¸­æŠ½å–è¯„è®ºæ•°æ®ï¼ˆä¸‹æ–¹å±•ç¤ºï¼‰')
    if st.checkbox('Show origin comments'):
        data_origin = load_data_origin()
        comments_origin = pd.DataFrame(data_origin.comment)
        st.dataframe(comments_origin, 500, 400)
    st.markdown('***')

if section == '2âƒ£ï¸ æ•°æ®é¢„å¤„ç†ä¸æ¢ç´¢':
    st.markdown('## 2. æ•°æ®é¢„å¤„ç†ä¸æ¢ç´¢')

    st.markdown('***')
    st.markdown('### 2.1 æ•°æ®å»é‡')
    st.markdown('è¯„è®ºæ•°æ®è¿›è¡Œå»é‡åï¼Œå…±è®¡ {} æ¡'.format(len(data_deldup)))
    if st.checkbox('Show delete duplicate comments'):
        comments_deldup = pd.DataFrame(data_deldup.comment)
        st.dataframe(comments_deldup, 500, 400)

    code_deldup = '''
    def del_duplicate(data):
    """
    å¯¹æ•°æ®æ¡†è¯„è®ºæ å»é‡
    :param data: (DataFrame)
    :return: (DataFrame)
    """
    result = data.drop_duplicates(['comment'], ignore_index=True)
    return result
    '''
    st.code(code_deldup)

    st.markdown('***')
    st.markdown('### 2.2 æœºæ¢°å‹ç¼©ä¸çŸ­å¥åˆ é™¤')
    st.markdown('è¯„è®ºæ•°æ®è¿›è¡Œæœºæ¢°å‹ç¼©ä¸çŸ­å¥åˆ é™¤åï¼Œå…±è®¡ {} æ¡'.format(len(data_compress)))
    if st.checkbox('Show compressed comments'):
        comments_compress = pd.DataFrame(data_compress.comment)
        st.dataframe(comments_compress, 500, 400)
    code_compress = '''
    def compress(comment, n=4):
    """
    å¯¹æ–‡æœ¬è¿›è¡Œæ­£åºã€é€†åºæœºæ¢°å‹ç¼©ï¼ŒçŸ­å¥åˆ é™¤çš„å¤„ç†
    :param n: æ‰€éœ€åˆ é™¤çŸ­å¥çš„é•¿åº¦
    :param comment: (str)
    :return: (str)
    """
    comp_comm = []
    comment = str(comment)
    if len(comment) <= n:
        pass
    else:
        comment = re.sub("[\s,./?'\"|\]\[{}+_)(*&^%$#@!~`=\-]+|[+\-ï¼Œã€‚/ï¼ï¼Ÿã€ï½@#Â¥%â€¦&*ï¼ˆï¼‰â€”ï¼›ï¼šâ€˜â€™â€œâ€]+", " ", comment)
        comp_list = list(comment)
        comp_list = compressed_text(comp_list)
        comp_list = compressed_text(comp_list[::-1])
        comp_list = comp_list[::-1]
        if len(comp_list) <= n:
            pass
        else:
            comp_comm = []
            comp_comm = (''.join(comp_list)).replace("\\n", "")
    return comp_comm
    '''
    st.code(code_compress)

    st.markdown('***')
    st.markdown('### 2.3 éšå«ä¿¡æ¯æŒ–æ˜ -- æ ‡ç­¾è¯„è®ºçš„è·å–')
    st.markdown('è§‚å¯Ÿè¯„è®ºæ•°æ®ï¼Œå‘ç°éƒ¨åˆ†è¯„è®ºæ•°æ®æœ‰å›ºå®šçš„ä¸»é¢˜æ ‡ç­¾ï¼Œå°†å…¶çˆ¬å–ä¸‹æ¥è¿›è¡Œå•†å“æŸæ–¹é¢ç‰¹å¾å®šå‘åˆ†æã€‚')
    if st.checkbox('Show tag comments'):
        st.json(tag_comment_origin)
    code_extract_tag = '''
    def extract_tag_comment(comment):
    """
    å¯¹è¯„è®ºæ•°æ®è¿›è¡Œæ ‡ç­¾è¯„è®ºæŠ½å–ï¼Œè‹¥ç¬¦åˆæ ‡ç­¾è¯„è®ºè§„èŒƒåˆ™è¿”å›æ ‡ç­¾è¯„è®ºå­—å…¸ï¼Œè‹¥ä¸ç¬¦åˆåˆ™è¿”å›None
    :param comment: (str)
    :return: (dict/None)
    """
    comment_lines = comment.split("\\n")
    tag_comment_dict = {}
    if len(comment) < 2:
        return None
    else:
        for lines in comment_lines:
            tag_comment = lines.split('ï¼š')
            if len(tag_comment) == 2:
                if len(tag_comment[0]) < 6:
                    tag_comment_dict[tag_comment[0]] = tag_comment[1]
            else:
                continue
    if tag_comment_dict:
        return tag_comment_dict
    else:
        return None
    '''
    st.code(code_extract_tag)

    st.markdown('***')
    st.markdown('### 2.4 æ•°æ®ç‰¹å¾æ¢ç´¢')
    st.markdown('#### 2.4.1 ç¾çš„çƒ­æ°´å™¨å•†å“è¯„è®ºçš„è¯„åˆ†åˆ†å¸ƒå›¾')
    score = pd.DataFrame(data_compress.score.value_counts())
    score.plot.pie(subplots=True)
    st.pyplot()

    st.markdown('#### 2.4.2 ç¾çš„çƒ­æ°´å™¨å•†å“è¯„åˆ†èµ°åŠ¿å›¾')
    date_score = pd.DataFrame(
        data_compress['score'].groupby(data_compress.creationDate.apply(lambda x: str(x)[:-3])).mean())
    # date_score.plot.line()
    # st.pyplot()
    st.line_chart(date_score)

    st.sidebar.markdown('# SparkClouds')
    kind = st.sidebar.selectbox('è¯·é€‰æ‹©è¦åˆ†æçš„è¯„è®ºè¯„åˆ†é«˜ä½', ['low score(è¯„åˆ†1ã€2ã€3)', 'high score(è¯„åˆ†4ã€5)'])
    st.markdown('#### 2.4.3 é«˜ä½åˆ†è¯„è®ºè¯é¢‘æ—¶é—´å˜åŒ–å›¾ (SparkClouds)')
    if kind == 'low score(è¯„åˆ†1ã€2ã€3)':
        image = load_image('data/source/lowscore_keywords.png')
        st.image(image, use_column_width=True)
    if kind == 'high score(è¯„åˆ†4ã€5)':
        image = load_image('data/source/highscore_keywords.png')
        st.image(image, use_column_width=True)
    st.markdown(
        'å‚è€ƒæ–‡çŒ®ï¼š[SparkClouds: Visualizing Trends in Tag Clouds](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613457)')
    st.markdown('***')

if section == '3âƒ£ï¸ è‡ªè®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹':
    st.markdown('## 3. è‡ªè®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹')

    st.markdown('***')
    st.markdown('### 3.1 æ¨¡å‹ç¥ç»ç½‘ç»œç»“æ„å›¾ç½‘ç»œ')

    b1 = st.checkbox('æ˜¾ç¤ºç»“æ„å›¾')
    if b1:
        file_ = open('data/Sentiment_Results/result_loss.gif', "rb")
        contents = file_.read()
        data_url = base64.b64encode(contents).decode("utf-8")
        file_.close()

        st.markdown(
            f'<img src="data:image/gif;base64,{data_url}" alt="cat gif" style="height:500px;">',
            unsafe_allow_html=True,
        )

    st.markdown('***')
    st.markdown('### 3.2 æ¨¡å‹è®­ç»ƒLossæ›²çº¿')

    b2 = st.checkbox('æ˜¾ç¤ºLossæ›²çº¿')
    if b2:
        file_ = open('data/Sentiment_Results/resulit_struction.gif', "rb")
        contents = file_.read()
        data_url = base64.b64encode(contents).decode("utf-8")
        file_.close()

        st.markdown(
            f'<img src="data:image/gif;base64,{data_url}" alt="cat gif" style="height:500px;">',
            unsafe_allow_html=True,
        )

    st.markdown('### 3.3æ¨¡å‹æ•ˆæœå¯¹æ¯”')
    df = pd.DataFrame({'Roberta-wwm': [0.9282735613010842, 0.9089481946624803, 0.9538714991762768],
                       'snowNLP': [0.87, 0.8680781758957655, 0.8766447368421053],
                       'ROSTCM6': [0.675, 0.6319612590799032, 0.8585526315789473]},
                      index=['Accuracy', 'Precision', 'Recall']
                      )
    st.table(df)
    # st.bar_chart(df.T)

    st.markdown('***')
    st.markdown('### 3.4 æƒ…æ„Ÿåˆ†æç»“æœåœ¨è¯„è®ºè¯„åˆ†ä¸­çš„åˆ†å¸ƒ')
    score_1 = data_comment[data_comment.score == 1].sentiment.value_counts()
    score_2 = data_comment[data_comment.score == 2].sentiment.value_counts()
    score_3 = data_comment[data_comment.score == 3].sentiment.value_counts()
    score_4 = data_comment[data_comment.score == 4].sentiment.value_counts()
    score_5 = data_comment[data_comment.score == 5].sentiment.value_counts()
    score = pd.DataFrame([score_1, score_2, score_3, score_4, score_5], index=[1, 2, 3, 4, 5])
    score = score.T
    score[1] = score[1] / (score.iloc[0, 0] + score.iloc[1, 0])
    score[2] = score[2] / (score.iloc[0, 1] + score.iloc[1, 1])
    score[3] = score[3] / (score.iloc[0, 2] + score.iloc[1, 2])
    score[4] = score[4] / (score.iloc[0, 3] + score.iloc[1, 3])
    score[5] = score[5] / (score.iloc[0, 4] + score.iloc[1, 4])
    score = score.T
    # score.plot.bar(stacked=True, color=['purple', 'skyblue'], alpha=0.5)
    # st.pyplot()
    score[0] = score[0] * -1
    st.bar_chart(score, width=100, use_container_width=True)
    st.markdown('***')

if section == '4âƒ£ï¸ è¯„è®ºåˆ†è¯ä¸æ”¹è¿›':
    st.markdown('## 4. è¯„è®ºåˆ†è¯ä¸æ”¹è¿›')

    st.markdown('***')
    st.markdown('### 4.1 æ€»ä½“è¯„è®ºåˆ†è¯')
    if st.checkbox('å±•ç¤ºæ€»ä½“ç§¯æè¯„è®ºåˆ†è¯'):
        st.dataframe(pos_comment)
    if st.checkbox('å±•ç¤ºæ€»ä½“æ¶ˆæè¯„è®ºåˆ†è¯'):
        st.dataframe(neg_comment)

    st.markdown('***')
    st.markdown('### 4.2 æ ‡ç­¾è¯„è®ºåˆ†è¯')
    if st.checkbox('å±•ç¤ºæ ‡ç­¾ç§¯æè¯„è®ºåˆ†è¯'):
        st.json(tag_comments_pos)
    if st.checkbox('å±•ç¤ºæ ‡ç­¾æ¶ˆæè¯„è®ºåˆ†è¯'):
        st.json(tag_comments_neg)

    st.markdown('***')
    st.markdown('### 4.3 è¯„è®ºåˆ†è¯æ”¹è¿›')
    code_cut_word = '''
    def str_cut(comment, stop_words):
    """
    åˆ‡åˆ†å­—ç¬¦ä¸²ï¼Œå¹¶å»é™¤åœç”¨è¯ï¼Œä¿ç•™v, n, a, d, vd, an, adï¼Œè¿”å›å¤„ç†å¥½çš„å•è¯
    :param comment: (str) æ–‡æœ¬è¯„è®º
    :param stop_words: (list) åœç”¨è¯
    :return: object_list (list) å¤„ç†å¥½çš„è¯
    """
    seg_list = psg.cut(comment)
    object_list = []
    for word in seg_list:  # å¾ªç¯è¯»å–æ¯ä¸ªåˆ†è¯
        # è·å¾—éœ€è¦çš„è¯æ€§ï¼Œå»é™¤åœç”¨è¯
        if word.word not in stop_words and (word.flag in ['v', 'n', 'a', 'd', 'vd', 'an', 'ad']): 
            object_list.append(word.word)  
    return object_list
    '''
    st.code(code_cut_word)
    st.markdown('***')

if section == '5âƒ£ï¸ è¯äº‘ä¸è¯­ä¹‰ç½‘ç»œæ„å»º':
    st.markdown('## 5. è¯äº‘ä¸è¯­ä¹‰ç½‘ç»œæ„å»º')

    st.markdown('***')
    st.markdown('### 5.1 ç¾çš„çƒ­æ°´å™¨å•†å“è¯„è®ºï¼ˆ{}ï¼‰çš„è¯äº‘'.format(tag))
    image = load_image('data/word_cloud/{}_pos.jpg'.format(tag))
    st.write('')
    st.image(image, caption='{} ç§¯ææƒ…æ„Ÿè¯äº‘'.format(tag), width=500)
    st.write('')
    image = load_image('data/word_cloud/{}_neg.jpg'.format(tag))
    st.image(image, caption='{} æ¶ˆææƒ…æ„Ÿè¯äº‘'.format(tag), width=500)
    st.write('')

    st.markdown('***')
    st.markdown('### 5.2 ç¾çš„çƒ­æ°´å™¨å•†å“è¯„è®ºï¼ˆ{}ï¼‰çš„è¯­ä¹‰ç½‘ç»œ'.format(tag))
    st.write('')
    image = load_image('data/word_net/{}_pos/net.jpg'.format(tag))
    st.image(image, caption='{} ç§¯ææƒ…æ„Ÿè¯­ä¹‰ç½‘ç»œ'.format(tag), use_column_width=True)
    st.write('')
    image = load_image('data/word_net/{}_neg/net.jpg'.format(tag))
    st.image(image, caption='{} æ¶ˆææƒ…æ„Ÿè¯­ä¹‰ç½‘ç»œ'.format(tag), use_column_width=True)
    st.write('')
    st.markdown('***')

if section == '6âƒ£ï¸ LDAä¸»é¢˜æ¨¡å‹æ„å»º':
    st.markdown('## 6. LDAä¸»é¢˜æ¨¡å‹æ„å»º')

    st.markdown('***')
    st.markdown('### 6.1 LDAå…³é”®å­—ä¸ä¸»é¢˜æå–\n')
    # if st.button('æ­£é¢è¯„è®ºLDAç»“æœå±•ç¤º'):
    #     login(u'lda_{}_{}_pos.html'.format(tag, topic_number))
    st.markdown(
        '### - [æ­£é¢è¯„è®ºLDAç»“æœå±•ç¤º](https://dyf-2316.github.io/LDA_Results/lda_{}_{}_pos.html)'.format(tag, topic_number))

    # if st.button('è´Ÿé¢è¯„è®ºLDAç»“æœå±•ç¤º'):
    #     login(u'lda_{}_{}_neg.html'.format(tag, topic_number))
    st.markdown(
        '### - [è´Ÿé¢è¯„è®ºLDAç»“æœå±•ç¤º](https://dyf-2316.github.io/LDA_Results/lda_{}_{}_neg.html)'.format(tag, topic_number))

    code_LDA = '''
    def LDA(data, components, htmlfile=None):
    """
    è®­ç»ƒLDAæ¨¡å‹ï¼ŒåŒæ—¶ç”Ÿæˆå¯è§†åŒ–æ–‡ä»¶
    :param data: (list) æ–‡æ¡£åˆ—è¡¨
    :param components: (int) æŒ‡å®šä¸»é¢˜æ•°  
    :param htmlfile: (str) å¯è§†åŒ–æ–‡ä»¶å­˜å‚¨è·¯å¾„
    :return: None
    """
    # å…³é”®è¯æå–å’Œå‘é‡è½¬åŒ–
    tf_vectorizer = CountVectorizer(max_features=1000,
                                    max_df=0.5,
                                    min_df=10,
                                    encoding='utf-8'
                                    )
    tf = tf_vectorizer.fit_transform(data)
    mylogger.debug('LDAæ¨¡å‹è®­ç»ƒå¼€å§‹')
    lda = LatentDirichletAllocation(n_components=components,
                                    max_iter=50,
                                    learning_method='online',
                                    learning_offset=50,
                                    random_state=0,
                                    )
    lda.fit(tf)
    mylogger.debug('LDAæ¨¡å‹è®­ç»ƒå®Œæˆ')
    result = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)
    pyLDAvis.save_html(result, '../data/LDA_Results/' + htmlfile)
    mylogger.debug('LDAæ¨¡å‹å¯è§†åŒ–æ–‡ä»¶å·²ç”Ÿæˆ')
    '''
    st.code(code_LDA)

    st.markdown('***')
    st.markdown('### 6.2 LDAæ¨¡å‹ä¼˜åŒ–')
    st.markdown('ç”¨ cv coherence æ¥åº¦é‡ä¸»é¢˜çš„è¿è´¯æ€§ï¼Œä»¥æ­¤æ¥é€‰æ‹©å‡ºæœ€ä¼˜çš„ ä¸»é¢˜æ•° ä½œä¸ºè¶…å‚æ•°')

    st.markdown('#### 6.2.1 {} æ­£é¢è¯„è®ºæ¨¡å‹æ•°ä¼˜åŒ–'.format(tag))
    data_cv_pos = pd.DataFrame([LDA_coherence[tag]['pos']], index=['cv'])
    data_cv_pos = data_cv_pos.T
    data_cv_pos = (data_cv_pos['cv'] - data_cv_pos['cv'].min()) / (data_cv_pos['cv'].max() - data_cv_pos['cv'].min())
    st.line_chart(data_cv_pos)

    st.markdown('#### 6.2.1 {} è´Ÿé¢è¯„è®ºæ¨¡å‹æ•°ä¼˜åŒ–'.format(tag))
    data_cv_neg = pd.DataFrame([LDA_coherence[tag]['neg']], index=['cv'])
    data_cv_neg = data_cv_neg.T
    data_cv_neg = (data_cv_neg['cv'] - data_cv_neg['cv'].min()) / (data_cv_neg['cv'].max() - data_cv_neg['cv'].min())
    st.line_chart(data_cv_neg)
    st.markdown('***')

if section == '7âƒ£ï¸ æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–':
    pass